{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-31 10:41:52,848\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "from vllm.utils import FlexibleArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_assets = [AudioAsset(\"mary_had_lamb\"), AudioAsset(\"winning_call\")]\n",
    "question_per_audio_count = {\n",
    "    0: \"What is 1+1?\",\n",
    "    1: \"What is recited in the audio?\",\n",
    "    2: \"What sport and what nursery rhyme are referenced?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultravox 0.3\n",
    "def run_ultravox(question: str, audio_count: int):\n",
    "    model_name = \"fixie-ai/ultravox-v0_3\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': \"<|audio|>\\n\" * audio_count + question\n",
    "    }]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "\n",
    "    llm = LLM(model=model_name,\n",
    "              max_model_len=4096,\n",
    "              max_num_seqs=5,\n",
    "              trust_remote_code=True,\n",
    "              limit_mm_per_prompt={\"audio\": audio_count})\n",
    "    stop_token_ids = None\n",
    "    return llm, prompt, stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2-Audio\n",
    "def run_qwen2_audio(question: str, audio_count: int):\n",
    "    model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
    "\n",
    "    llm = LLM(model=model_name,\n",
    "              max_model_len=4096,\n",
    "              max_num_seqs=5,\n",
    "              limit_mm_per_prompt={\"audio\": audio_count})\n",
    "\n",
    "    audio_in_prompt = \"\".join([\n",
    "        f\"Audio {idx+1}: \"\n",
    "        f\"<|audio_bos|><|AUDIO|><|audio_eos|>\\n\" for idx in range(audio_count)\n",
    "    ])\n",
    "\n",
    "    prompt = (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "              \"<|im_start|>user\\n\"\n",
    "              f\"{audio_in_prompt}{question}<|im_end|>\\n\"\n",
    "              \"<|im_start|>assistant\\n\")\n",
    "    stop_token_ids = None\n",
    "    return llm, prompt, stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minicpmo(question: str, audio_count: int):\n",
    "    model_name = \"openbmb/MiniCPM-o-2_6\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                              trust_remote_code=True)\n",
    "    llm = LLM(model=model_name,\n",
    "              trust_remote_code=True,\n",
    "              max_model_len=4096,\n",
    "              max_num_seqs=5,\n",
    "              limit_mm_per_prompt={\"audio\": audio_count})\n",
    "\n",
    "    stop_tokens = ['<|im_end|>', '<|endoftext|>']\n",
    "    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n",
    "\n",
    "    audio_placeholder = \"(<audio>./</audio>)\" * audio_count\n",
    "    audio_chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n<|spk_bos|><|spk|><|spk_eos|><|tts_bos|>' }}{% endif %}\"  # noqa: E501\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': f'{audio_placeholder}\\n{question}'\n",
    "    }]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True,\n",
    "                                           chat_template=audio_chat_template)\n",
    "    return llm, prompt, stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_example_map = {\n",
    "    \"ultravox\": run_ultravox,\n",
    "    \"qwen2_audio\": run_qwen2_audio,\n",
    "    \"minicpmo\": run_minicpmo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    model = \"ultravox\"\n",
    "    if model not in model_example_map:\n",
    "        raise ValueError(f\"Model type {model} is not supported.\")\n",
    "\n",
    "    audio_count = 1\n",
    "    llm, prompt, stop_token_ids = model_example_map[model](\n",
    "        question_per_audio_count[audio_count], audio_count)\n",
    "\n",
    "    # We set temperature to 0.2 so that outputs can be different\n",
    "    # even when all prompts are identical when running batch inference.\n",
    "    sampling_params = SamplingParams(temperature=0.2,\n",
    "                                     max_tokens=64,\n",
    "                                     stop_token_ids=stop_token_ids)\n",
    "\n",
    "    mm_data = {}\n",
    "    if audio_count > 0:\n",
    "        mm_data = {\n",
    "            \"audio\": [\n",
    "                asset.audio_and_sample_rate\n",
    "                for asset in audio_assets[:audio_count]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    assert args.num_prompts > 0\n",
    "    inputs = {\"prompt\": prompt, \"multi_modal_data\": mm_data}\n",
    "    if args.num_prompts > 1:\n",
    "        # Batch inference\n",
    "        inputs = [inputs] * args.num_prompts\n",
    "\n",
    "    outputs = llm.generate(inputs, sampling_params=sampling_params)\n",
    "\n",
    "    for o in outputs:\n",
    "        generated_text = o.outputs[0].text\n",
    "        print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 10:42:02 config.py:510] This model supports multiple tasks: {'score', 'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-31 10:42:02 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='fixie-ai/ultravox-v0_3', speculative_config=None, tokenizer='fixie-ai/ultravox-v0_3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=fixie-ai/ultravox-v0_3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[8,4,2,1],\"max_capture_size\":8}, use_cached_outputs=False, \n",
      "INFO 01-31 10:42:04 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-31 10:42:04 model_runner.py:1094] Starting to load model fixie-ai/ultravox-v0_3...\n",
      "INFO 01-31 10:42:05 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"model_type\": \"ultravox\",\n",
    "    \"num_prompts\": 1,\n",
    "    \"num_audios\": 1\n",
    "}\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
